# Feature Specification: Workflow Refactor to Python-Orchestrated Pattern

**Feature Branch**: `020-workflow-refactor`
**Created**: 2025-12-12
**Status**: Draft
**Input**: User description: "Refactor FlyWorkflow and RefuelWorkflow to use Python-orchestrated pattern where Python handles deterministic actions and Claude handles judgment"

## User Scenarios & Testing *(mandatory)*

### User Story 1 - Run FlyWorkflow with Reduced Token Usage (Priority: P1)

As a developer using Maverick, I want the FlyWorkflow to complete feature implementations with significantly fewer AI tokens consumed, so that my workflow costs are reduced and completion is faster.

**Why this priority**: Token efficiency is the primary driver for this refactor. Reducing token usage by 40-60% directly impacts operational costs and workflow speed.

**Independent Test**: Can be fully tested by running a complete FlyWorkflow from task file to PR creation and comparing token consumption against the baseline implementation.

**Acceptance Scenarios**:

1. **Given** a valid task file and spec directory, **When** FlyWorkflow executes the INIT stage, **Then** branch creation and task parsing happen without any AI tool calls
2. **Given** the IMPLEMENTATION stage completes, **When** the COMMIT stage runs, **Then** commit message generation uses a focused AI call while git operations are handled directly
3. **Given** a complete FlyWorkflow run, **When** comparing token usage to the previous implementation, **Then** token consumption is reduced by at least 40%

---

### User Story 2 - Run RefuelWorkflow for Multiple Issues (Priority: P1)

As a developer using Maverick, I want the RefuelWorkflow to process multiple GitHub issues in sequence with proper isolation, so that tech-debt issues are resolved reliably without cross-contamination.

**Why this priority**: RefuelWorkflow is a core automation capability. Reliable multi-issue processing with proper branch/commit isolation is essential.

**Independent Test**: Can be fully tested by creating 3 test issues, running RefuelWorkflow, and verifying each produces its own branch, commits, and PR.

**Acceptance Scenarios**:

1. **Given** multiple open issues with the target label, **When** RefuelWorkflow starts, **Then** issues are fetched using direct process calls without AI involvement
2. **Given** an issue to process, **When** the workflow creates a branch, **Then** branch naming follows the pattern `fix/issue-{number}` and is created via direct git operations
3. **Given** issue fixes are implemented, **When** the workflow creates a PR, **Then** PR body is generated by AI while PR creation uses direct process calls

---

### User Story 3 - Test Workflows with Mocked Runners (Priority: P2)

As a developer maintaining Maverick, I want to write tests for workflows using mocked runner components, so that I can verify workflow logic without executing real git commands or AI calls.

**Why this priority**: Testability enables confidence in changes and faster development cycles. Mocked runners are essential for comprehensive unit testing.

**Independent Test**: Can be fully tested by writing a workflow test that uses mocked GitRunner, ValidationRunner, and GithubRunner to verify stage sequencing.

**Acceptance Scenarios**:

1. **Given** a test environment with mocked runners, **When** FlyWorkflow is instantiated, **Then** all runners are injectable through the constructor
2. **Given** mocked runners with predefined responses, **When** a workflow stage executes, **Then** the workflow uses the mocked responses and proceeds accordingly
3. **Given** a mocked ValidationRunner returning failures, **When** the validation loop runs, **Then** the workflow correctly invokes the fixer agent and retries validation

---

### User Story 4 - Monitor Workflow Progress at Each Stage (Priority: P2)

As a developer using Maverick, I want to see progress updates at each deterministic step of the workflow, so that I understand exactly where the workflow is in execution.

**Why this priority**: Progress visibility improves user experience and aids debugging. Python-controlled stages enable granular progress reporting.

**Independent Test**: Can be fully tested by subscribing to workflow progress events and verifying each stage transition emits appropriate updates.

**Acceptance Scenarios**:

1. **Given** a workflow in progress, **When** each stage completes, **Then** a progress update is emitted with the stage name and status
2. **Given** the validation loop is running, **When** each iteration completes, **Then** a progress update shows the attempt number and validation result
3. **Given** an agent is executing, **When** the agent returns, **Then** a progress update indicates agent completion with summary information

---

### User Story 5 - Agent Tool Permissions are Properly Scoped (Priority: P3)

As a system administrator, I want agents to have minimal required permissions, so that security risks are reduced and agents cannot perform unintended actions.

**Why this priority**: Principle of least privilege is a security best practice. Properly scoped permissions prevent accidental or malicious misuse.

**Independent Test**: Can be fully tested by inspecting agent configurations and verifying each agent's allowed_tools list matches the defined permissions.

**Acceptance Scenarios**:

1. **Given** the ImplementerAgent configuration, **When** examining allowed tools, **Then** only file manipulation and local execution tools are permitted (Read, Write, Edit, Bash, Glob, Grep) - no git or GitHub tools
2. **Given** the CodeReviewerAgent configuration, **When** examining allowed tools, **Then** only file inspection tools are permitted (Read, Glob, Grep, Bash for running tests)
3. **Given** any agent attempting an unauthorized tool call, **When** the tool is not in allowed_tools, **Then** the tool call is rejected

---

### Edge Cases

- What happens when git branch creation fails due to name conflict? → Append timestamp suffix and continue (FR-001a)
- What happens when validation fails more than the maximum allowed attempts? → Continue workflow, mark PR as draft (FR-009a)
- What happens when an issue has no clear fix and the agent cannot make progress? → Mark as skipped after 3 attempts (FR-026)
- How does the system handle network failures during GitHub API calls? → Retry with exponential backoff (FR-025)
- What happens when CodeRabbit is unavailable during code review stage? → Skip with warning, continue workflow (FR-010a)
- How does the system handle partial completion (some issues succeed, some fail)? → Aggregate with success/failure/skipped counts (FR-027)

## Requirements *(mandatory)*

### Functional Requirements

#### FlyWorkflow Requirements

- **FR-001**: System MUST create feature branches using direct process execution without AI tool calls
- **FR-001a**: System MUST append a timestamp suffix to branch name when a name conflict occurs, allowing workflow to continue
- **FR-002**: System MUST read and parse task files using direct file operations without AI tool calls
- **FR-003**: System MUST build implementation context by aggregating file contents, task definitions, and project conventions
- **FR-004**: System MUST delegate code implementation to an agent with only file manipulation tools (Read, Write, Edit, Glob, Grep)
- **FR-005**: System MUST generate commit messages using a focused AI call with the git diff as context
- **FR-006**: System MUST execute git commit operations using direct process execution
- **FR-007**: System MUST run validation stages (format, lint, build, test) using direct process execution
- **FR-008**: System MUST delegate validation fixes to an agent when validation fails, limited to file modification tools
- **FR-009**: System MUST retry validation up to a configurable maximum number of attempts (default: 3)
- **FR-009a**: System MUST continue workflow when validation exhausts retries, marking the resulting PR as draft
- **FR-010**: System MUST run code review tools (CodeRabbit) using direct process execution
- **FR-010a**: System MUST skip code review with a warning when CodeRabbit is unavailable, allowing workflow to continue
- **FR-011**: System MUST delegate code review interpretation to an agent that receives external review findings as context
- **FR-012**: System MUST generate PR descriptions using a focused AI call with workflow summary as context
- **FR-013**: System MUST create pull requests using direct process execution

#### RefuelWorkflow Requirements

- **FR-014**: System MUST fetch issues from GitHub using direct process execution without AI tool calls
- **FR-015**: System MUST create issue-specific branches using the pattern `fix/issue-{number}`
- **FR-016**: System MUST build issue context including issue title, body, labels, and related files
- **FR-017**: System MUST delegate issue fixes to an agent with only file manipulation tools
- **FR-018**: System MUST process multiple issues sequentially with proper isolation between each
- **FR-019**: System MUST aggregate results from all processed issues into a summary report

#### Cross-Cutting Requirements

- **FR-020**: All runners (GitRunner, ValidationRunner, GithubRunner, CodeRabbitRunner) MUST be injectable as dependencies
- **FR-021**: All agent executions MUST use explicitly defined allowed_tools lists
- **FR-022**: System MUST emit progress updates at each stage transition
- **FR-023**: System MUST capture and report errors without crashing the entire workflow
- **FR-024**: System MUST support dry-run mode where runners log operations instead of executing them
- **FR-025**: System MUST retry GitHub API operations with exponential backoff (3 attempts, 1s/2s/4s delays) on network failures before failing the operation
- **FR-026**: System MUST mark an issue as skipped after 3 unsuccessful agent fix attempts, logging context for manual review
- **FR-027**: System MUST aggregate partial completion results, reporting per-issue success/failure/skipped counts in the final summary

### Key Entities

- **Workflow Stage**: Represents a discrete phase in workflow execution (name, type: python-only | claude | mixed, status, progress data)
- **Runner**: Abstraction for external system interaction (git, validation, github, coderabbit), injectable and mockable
- **Agent Context**: Structured input for agent execution containing task definition, relevant files, constraints, and external findings
- **Validation Result**: Outcome of running validation stages (success flag, error messages, affected files)
- **Workflow Summary**: Aggregated data from workflow execution used for PR body generation

## Success Criteria *(mandatory)*

### Measurable Outcomes

- **SC-001**: Workflow completion uses 40-60% fewer AI tokens compared to the baseline implementation
- **SC-002**: All deterministic operations (git commands, file reads, process execution) complete without AI tool calls
- **SC-003**: 100% of workflow tests pass using mocked runners without any real external calls
- **SC-004**: Each workflow stage emits at least one progress update visible to the user
- **SC-005**: Agent tool permissions are enforced; unauthorized tool calls are rejected 100% of the time
- **SC-006**: Workflow failures are contained to individual stages or issues; one failure does not crash the entire workflow
- **SC-007**: FlyWorkflow completes a typical feature implementation end-to-end successfully
- **SC-008**: RefuelWorkflow processes at least 3 issues in a single run with proper isolation between each

## Clarifications

### Session 2025-12-18

- Q: What is the default validation retry limit? → A: Default 3 retries (balanced approach)
- Q: What happens when CodeRabbit is unavailable? → A: Skip code review with warning, continue workflow
- Q: What happens when validation exhausts all retries? → A: Fail current stage, continue workflow, mark PR as draft
- Q: What happens when branch creation fails due to name conflict? → A: Append timestamp suffix and continue

## Assumptions

- The existing FlyWorkflow and RefuelWorkflow implementations provide reference logic for the refactored versions
- The Claude Agent SDK supports explicit allowed_tools configuration for all agent executions
- External tools (git, gh CLI, CodeRabbit CLI) are available in the execution environment
- Validation stages (format, lint, build, test) have deterministic outcomes based on code state
- Context builders can aggregate relevant information without AI assistance
- Commit message and PR body generation are suitable tasks for focused, single-purpose AI calls
